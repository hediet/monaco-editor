import*as arrays from"../../../base/common/arrays.js";import{onUnexpectedError}from"../../../base/common/errors.js";import{LineTokens}from"../tokens/lineTokens.js";import{TokenizationRegistry}from"../languages.js";import{nullTokenizeEncoded}from"../languages/nullTokenize.js";import{Disposable}from"../../../base/common/lifecycle.js";import{StopWatch}from"../../../base/common/stopwatch.js";import{countEOL}from"../core/eolCounter.js";import{ContiguousMultilineTokensBuilder}from"../tokens/contiguousMultilineTokensBuilder.js";import{runWhenIdle}from"../../../base/common/async.js";import{setTimeout0}from"../../../base/common/platform.js";class ContiguousGrowingArray{constructor(t){this._default=t,this._store=[]}get(t){return t<this._store.length?this._store[t]:this._default}set(t,e){for(;t>=this._store.length;)this._store[this._store.length]=this._default;this._store[t]=e}delete(t,e){0===e||t>=this._store.length||this._store.splice(t,e)}insert(t,e){if(0===e||t>=this._store.length)return;const i=[];for(let t=0;t<e;t++)i[t]=this._default;this._store=arrays.arrayInsert(this._store,t,i)}}export class TokenizationStateStore{get invalidLineStartIndex(){return this._firstLineNeedsTokenization}constructor(t,e){this.tokenizationSupport=t,this.initialState=e,this._lineBeginState=new ContiguousGrowingArray(null),this._lineNeedsTokenization=new ContiguousGrowingArray(!0),this._firstLineNeedsTokenization=0,this._lineBeginState.set(0,this.initialState)}markMustBeTokenized(t){this._lineNeedsTokenization.set(t,!0),this._firstLineNeedsTokenization=Math.min(this._firstLineNeedsTokenization,t)}getBeginState(t){return this._lineBeginState.get(t)}setEndState(t,e,i){if(this._lineNeedsTokenization.set(e,!1),this._firstLineNeedsTokenization=e+1,e===t-1)return;const n=this._lineBeginState.get(e+1);if(null===n||!i.equals(n))return this._lineBeginState.set(e+1,i),void this.markMustBeTokenized(e+1);let o=e+1;for(;o<t&&!this._lineNeedsTokenization.get(o);)o++;this._firstLineNeedsTokenization=o}applyEdits(t,e){this.markMustBeTokenized(t.startLineNumber-1),this._lineBeginState.delete(t.startLineNumber,t.endLineNumber-t.startLineNumber),this._lineNeedsTokenization.delete(t.startLineNumber,t.endLineNumber-t.startLineNumber),this._lineBeginState.insert(t.startLineNumber,e),this._lineNeedsTokenization.insert(t.startLineNumber,e)}}export class TextModelTokenization extends Disposable{constructor(t,e,i){super(),this._textModel=t,this._tokenizationPart=e,this._languageIdCodec=i,this._isScheduled=!1,this._isDisposed=!1,this._tokenizationStateStore=null,this._register(TokenizationRegistry.onDidChange((t=>{const e=this._textModel.getLanguageId();-1!==t.changedLanguages.indexOf(e)&&(this._resetTokenizationState(),this._tokenizationPart.clearTokens())}))),this._resetTokenizationState()}dispose(){this._isDisposed=!0,super.dispose()}handleDidChangeContent(t){if(t.isFlush)this._resetTokenizationState();else{if(this._tokenizationStateStore)for(let e=0,i=t.changes.length;e<i;e++){const i=t.changes[e],[n]=countEOL(i.text);this._tokenizationStateStore.applyEdits(i.range,n)}this._beginBackgroundTokenization()}}handleDidChangeAttached(){this._beginBackgroundTokenization()}handleDidChangeLanguage(t){this._resetTokenizationState(),this._tokenizationPart.clearTokens()}_resetTokenizationState(){const[t,e]=initializeTokenization(this._textModel,this._tokenizationPart);this._tokenizationStateStore=t&&e?new TokenizationStateStore(t,e):null,this._beginBackgroundTokenization()}_beginBackgroundTokenization(){!this._isScheduled&&this._textModel.isAttachedToEditor()&&this._hasLinesToTokenize()&&(this._isScheduled=!0,runWhenIdle((t=>{this._isScheduled=!1,this._backgroundTokenizeWithDeadline(t)})))}_backgroundTokenizeWithDeadline(t){const e=Date.now()+t.timeRemaining(),i=()=>{!this._isDisposed&&this._textModel.isAttachedToEditor()&&this._hasLinesToTokenize()&&(this._backgroundTokenizeForAtLeast1ms(),Date.now()<e?setTimeout0(i):this._beginBackgroundTokenization())};i()}_backgroundTokenizeForAtLeast1ms(){const t=this._textModel.getLineCount(),e=new ContiguousMultilineTokensBuilder,i=StopWatch.create(!1);do{if(i.elapsed()>1)break;if(this._tokenizeOneInvalidLine(e)>=t)break}while(this._hasLinesToTokenize());this._tokenizationPart.setTokens(e.finalize(),this._isTokenizationComplete())}tokenizeViewport(t,e){const i=new ContiguousMultilineTokensBuilder;this._tokenizeViewport(i,t,e),this._tokenizationPart.setTokens(i.finalize(),this._isTokenizationComplete())}reset(){this._resetTokenizationState(),this._tokenizationPart.clearTokens()}forceTokenization(t){const e=new ContiguousMultilineTokensBuilder;this._updateTokensUntilLine(e,t),this._tokenizationPart.setTokens(e.finalize(),this._isTokenizationComplete())}getTokenTypeIfInsertingCharacter(t,e){if(!this._tokenizationStateStore)return 0;this.forceTokenization(t.lineNumber);const i=this._tokenizationStateStore.getBeginState(t.lineNumber-1);if(!i)return 0;const n=this._textModel.getLanguageId(),o=this._textModel.getLineContent(t.lineNumber),s=o.substring(0,t.column-1)+e+o.substring(t.column-1),a=safeTokenize(this._languageIdCodec,n,this._tokenizationStateStore.tokenizationSupport,s,!0,i),r=new LineTokens(a.tokens,s,this._languageIdCodec);if(0===r.getCount())return 0;const l=r.findTokenIndexAtOffset(t.column-1);return r.getStandardTokenType(l)}tokenizeLineWithEdit(t,e,i){const n=t.lineNumber,o=t.column;if(!this._tokenizationStateStore)return null;this.forceTokenization(n);const s=this._tokenizationStateStore.getBeginState(n-1);if(!s)return null;const a=this._textModel.getLineContent(n),r=a.substring(0,o-1)+i+a.substring(o-1+e),l=this._textModel.getLanguageIdAtPosition(n,0),h=safeTokenize(this._languageIdCodec,l,this._tokenizationStateStore.tokenizationSupport,r,!0,s);return new LineTokens(h.tokens,r,this._languageIdCodec)}isCheapToTokenize(t){if(!this._tokenizationStateStore)return!0;const e=this._tokenizationStateStore.invalidLineStartIndex+1;return!(t>e)&&(t<e||this._textModel.getLineLength(t)<2048)}_hasLinesToTokenize(){return!!this._tokenizationStateStore&&this._tokenizationStateStore.invalidLineStartIndex<this._textModel.getLineCount()}_isTokenizationComplete(){return!!this._tokenizationStateStore&&this._tokenizationStateStore.invalidLineStartIndex>=this._textModel.getLineCount()}_tokenizeOneInvalidLine(t){if(!this._tokenizationStateStore||!this._hasLinesToTokenize())return this._textModel.getLineCount()+1;const e=this._tokenizationStateStore.invalidLineStartIndex+1;return this._updateTokensUntilLine(t,e),e}_updateTokensUntilLine(t,e){if(!this._tokenizationStateStore)return;const i=this._textModel.getLanguageId(),n=this._textModel.getLineCount(),o=e-1;for(let e=this._tokenizationStateStore.invalidLineStartIndex;e<=o;e++){const o=this._textModel.getLineContent(e+1),s=this._tokenizationStateStore.getBeginState(e),a=safeTokenize(this._languageIdCodec,i,this._tokenizationStateStore.tokenizationSupport,o,!0,s);t.add(e+1,a.tokens),this._tokenizationStateStore.setEndState(n,e,a.endState),e=this._tokenizationStateStore.invalidLineStartIndex-1}}_tokenizeViewport(t,e,i){if(!this._tokenizationStateStore)return;if(i<=this._tokenizationStateStore.invalidLineStartIndex)return;if(e<=this._tokenizationStateStore.invalidLineStartIndex)return void this._updateTokensUntilLine(t,i);let n=this._textModel.getLineFirstNonWhitespaceColumn(e);const o=[];let s=null;for(let t=e-1;n>1&&t>=1;t--){const e=this._textModel.getLineFirstNonWhitespaceColumn(t);if(0!==e&&e<n&&(o.push(this._textModel.getLineContent(t)),n=e,s=this._tokenizationStateStore.getBeginState(t-1),s))break}s||(s=this._tokenizationStateStore.initialState);const a=this._textModel.getLanguageId();let r=s;for(let t=o.length-1;t>=0;t--)r=safeTokenize(this._languageIdCodec,a,this._tokenizationStateStore.tokenizationSupport,o[t],!1,r).endState;for(let n=e;n<=i;n++){const e=this._textModel.getLineContent(n),i=safeTokenize(this._languageIdCodec,a,this._tokenizationStateStore.tokenizationSupport,e,!0,r);t.add(n,i.tokens),this._tokenizationStateStore.markMustBeTokenized(n-1),r=i.endState}}}function initializeTokenization(t,e){if(t.isTooLargeForTokenization())return[null,null];const i=TokenizationRegistry.get(e.getLanguageId());if(!i)return[null,null];let n;try{n=i.getInitialState()}catch(t){return onUnexpectedError(t),[null,null]}return[i,n]}function safeTokenize(t,e,i,n,o,s){let a=null;if(i)try{a=i.tokenizeEncoded(n,o,s.clone())}catch(t){onUnexpectedError(t)}return a||(a=nullTokenizeEncoded(t.encodeLanguageId(e),s)),LineTokens.convertToEndOffset(a.tokens,n.length),a}